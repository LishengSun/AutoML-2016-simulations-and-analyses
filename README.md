This documents aims to explaining how to use the provided scripts to generate the tables / figures in my AutoML benchmark report (https://www.overleaf.com/9694794sjksvwspnnfc).

I) Table 1: Performance of winner solutions on all 30 datasets
This table reports the task-specific scores of winner solutions applied on all 30 datasets. 

I.1) The predictions are saved in simulation_results/res/. To reproduce them, you can unzip winner_submission.zip and go to a certain winner submission folder, execute run.py under env "Codalab-AutoML-env" (see "create_Codalab-AutoML-env_on_tipi.txt" for instructions)).

I.2) The scores are saved in *************************. To reproduce these scores, edit scoring_program/score.py so that it points to the right prediction files (see instructions in score.py for more detail) and execute it.

I.3) For aad_freiburg (i.e. auto-sklearn), there are 2 scores: one achieved within the task-specific time budget, the other achieved with much longer learning process. The corresponding prediction files are stored in simulation_results/res/aad_freiburg/aad_freiburg_timebudget/ and simulation_results/res/aad_freiburg/aad_freiburg_global/



II) Table 2: same than Table 1, with error bars

Error bars are computed using scoring_program/error_bar.py: resample without replacement 10% subset of test set, repeat 100 times.


III) Learning curve:

Generated by learning_curves/plot_learning_curve.py: by default uses log time, can set to normal time.

All curves shown in the report are stored in learning_curves/LearningCurvePng/.


